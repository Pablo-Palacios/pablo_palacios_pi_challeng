{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92de8ca4-7181-45f5-9980-b723d2566f5e",
   "metadata": {},
   "source": [
    "# Challenge Prompting\n",
    "\n",
    "Resolver los siguientes ejercicios dejando el codigo con su ejecucion.\n",
    "\n",
    "Importar las librerias necesarias y **correr las celdas para visualizar el resultado en cada ejercicio**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65bbb09-0f5a-4e97-9a06-2361c5cdd1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## bloque importacion de librerias\n",
    "\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e004e5b7-b704-4592-b8b4-b01b8d6687cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTa138QUrWAZ74XpT2d8t4FWYwFc4NF5mF0Z9x81\n"
     ]
    }
   ],
   "source": [
    "## bloque variables de entorno\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load .env file\n",
    "\n",
    "api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "print(api_key)  # Verify the key is loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6745ff-cd45-4231-b3d6-518954a9ffca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='0ef7c213-cb0f-45b4-8640-4328bb780dd1' finish_reason='COMPLETE' prompt=None message=AssistantMessageResponse(role='assistant', tool_calls=None, tool_plan=None, content=[TextAssistantMessageResponseContentItem(type='text', text='Hello! How can I help you today?')], citations=None) usage=Usage(billed_units=UsageBilledUnits(input_tokens=3.0, output_tokens=9.0, search_units=None, classifications=None), tokens=UsageTokens(input_tokens=204.0, output_tokens=9.0)) logprobs=None\n"
     ]
    }
   ],
   "source": [
    "## bloque conexion a Cohere\n",
    "import cohere\n",
    "\n",
    "co = cohere.ClientV2()\n",
    "# alternativa:\n",
    "# co = cohere.ClientV2(api_key)\n",
    "\n",
    "response = co.chat(\n",
    "    model=\"command-r-plus-08-2024\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"hello world!\"}],\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aacdb26-ce51-49cc-b37f-5aa45c09ff51",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "Extraccion de entidades\n",
    "\n",
    "Utilizar el LLM para extraer las siguientes entidades del texto medico.\n",
    "\n",
    "- Paciente:\n",
    "    - Nombre\n",
    "    - Edad\n",
    "- Fecha de admisi√≥n\n",
    "- S√≠ntomas\n",
    "- Diagn√≥stico\n",
    "- Tratamiento recomendado\n",
    "\n",
    "**Aclaracion:** \n",
    "\n",
    "La salida tiene que ser un **string con formato de tipo json**, el cual se convertira en un diccionario de Python.\n",
    "\n",
    "Si la linea de conversion en test da error el ejercicio no esta completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e43d7-b074-4973-9cd0-5a6fbe816084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo \n",
    "\n",
    "# texto a analizar\n",
    "\"\"\"La paciente, Mar√≠a Gonz√°lez, de 45 a√±os, fue admitida en el Hospital Central el 5 de agosto de 2023 debido a s√≠ntomas de fatiga cr√≥nica y dolores musculares./\n",
    "Tras una serie de an√°lisis, se diagnostic√≥ fibromialgia. La doctora a cargo, Laura Ram√≠rez, recomend√≥ un tratamiento basado en fisioterapia y medicamentos analg√©sicos. /\n",
    "La pr√≥xima consulta est√° programada para el 15 de septiembre.\"\"\"\n",
    "\n",
    "\n",
    "# respuesta del LLM\n",
    "{\n",
    "  \"paciente\": {\n",
    "    \"nombre\": \"Mar√≠a Gonz√°lez\",\n",
    "    \"edad\": 45\n",
    "  },\n",
    "  \"fecha_admision\": \"2023-08-05\",\n",
    "  \"sintomas\": [\n",
    "    \"fatiga cr√≥nica\",\n",
    "    \"dolores musculares\"\n",
    "  ],\n",
    "  \"diagnostico\": \"fibromialgia\",\n",
    "  \"tratamiento\": [\n",
    "    \"fisioterapia\",\n",
    "    \"medicamentos analg√©sicos\"\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac8d0a6-db5c-4eec-9f71-89e60ceaf914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "425cffab-9efd-4d80-bc64-6ef69ce233e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_analize = \"\"\"Sof√≠a L√≥pez, de 28 a√±os, ingres√≥ al Hospital Infantil el 3 de abril de 2023 debido a fiebre alta y tos persistente./\n",
    "Despu√©s de varias pruebas, se le diagnostic√≥ neumon√≠a. La pediatra responsable, Dra. Claudia Torres, indic√≥ tratamiento con antibi√≥ticos y reposo./\n",
    "La pr√≥xima evaluaci√≥n ser√° el 10 de abril.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ff6d292-d4cb-4484-811e-d0d641c66d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"paciente\": {\n",
      "    \"nombre\": \"Sofia Lopez\",\n",
      "    \"edad\": \"28 a√±os\"\n",
      "  },\n",
      "  \"fecha_ingreso\": \"3 de abril de 2023\",\n",
      "  \"sintomas\": [\n",
      "    \"Fiebre alta\",\n",
      "    \"Tos persistente\"\n",
      "  ],\n",
      "  \"diagnostico\": \"Neumon√≠a\",\n",
      "  \"tratamiento\": [\n",
      "    \"Antibi√≥ticos\",\n",
      "    \"Reposo\"\n",
      "  ],\n",
      "  \"fecha_seguimiento\": \"10 de abril de 2023\"\n",
      "}\n",
      "{'paciente': {'nombre': 'Sofia Lopez', 'edad': '28 a√±os'}, 'fecha_ingreso': '3 de abril de 2023', 'sintomas': ['Fiebre alta', 'Tos persistente'], 'diagnostico': 'Neumon√≠a', 'tratamiento': ['Antibi√≥ticos', 'Reposo'], 'fecha_seguimiento': '10 de abril de 2023'}\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import cohere\n",
    "\n",
    "co = cohere.ClientV2()\n",
    "\n",
    "\n",
    "text_to_analize = \"\"\"Sof√≠a L√≥pez, de 28 a√±os, ingres√≥ al Hospital Infantil el 3 de abril de 2023 debido a fiebre alta y tos persistente./\n",
    "Despu√©s de varias pruebas, se le diagnostic√≥ neumon√≠a. La pediatra responsable, Dra. Claudia Torres, indic√≥ tratamiento con antibi√≥ticos y reposo./\n",
    "La pr√≥xima evaluaci√≥n ser√° el 10 de abril.\"\"\"\n",
    "\n",
    "responses = co.chat(\n",
    "    model = \"command\",\n",
    "    messages = [{\"role\":\"system\", \"content\":\"Eres un asistente virtual medico, capaz de generar resumenes sobre informes medicos.\"},\n",
    "                {\"role\":\"user\", \"content\":f\"Sobre el informe {text_to_analize}, debes reconocer y ordenar los siguientes datos: datos personales del paciente (nombre y edad), su fecha de admision, sintomas, diagnostico y tratamiento. {text_to_analize}\"}]\n",
    ")\n",
    "body = responses.message.content[0].text\n",
    "#print(body)\n",
    "\n",
    "resp_light = co.chat(\n",
    "    model = \"command-r-plus-08-2024\",\n",
    "    messages = [{\"role\":\"system\", \"content\":\"Eres un asistente virtual especializado en conversion de datos en formato json, solo datos relevantes. Si vez algun campo con mas de un dato, generalo como una lista. Ademas, el contenido debe ser en el idioma espa√±ol\"},\n",
    "                {\"role\":\"user\", \"content\":f\"Convierte esta informacion en una respuesta json: {body}.\"}]\n",
    ")\n",
    "\n",
    "llm_response = resp_light.message.content[0].text\n",
    "print(llm_response)\n",
    "\n",
    "#test\n",
    "final_result = json.loads(llm_response)\n",
    "\n",
    "print(final_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ee190-faca-46d8-9c11-0f8904bd1752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e72e5-3bd5-4f26-860c-340e25e72f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b37fbf25-6db4-432a-82c4-2e7edce27686",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Tenemos dos funciones en Python, una llamada *'add_contact'* y otra llamada *'get_information'*.\n",
    "\n",
    "**Utilizar algun LLM que permita funtion calling** y desarrollar un codigo secuencial automatico que consiga:\n",
    "\n",
    "Interpretar la consulta del usuario, identificar a que funcion llamar, luego llamarla (si es que aplica) y darle una respuesta final al usuario.  (usar function calling para esta solucion)\n",
    "\n",
    "La entrada a dicho codigo es la consulta del usuario, a continuacion algunos ejemplos:\n",
    "\n",
    "- \"Agrega a Juan P√©rez con el n√∫mero 555-1234 y el correo juanperez@mail.com.\"\n",
    "- \"Guarda a Luc√≠a G√≥mez en mis contactos. Su tel√©fono es 555-5678 y su email es lucia.gomez@gmail.com.\"\n",
    "- \"Cual es el Email de Juan P√©rez.?\"\n",
    "\n",
    "Salidas esperadas de dichos ejemplos (variaran porque las genera el LLM):\n",
    "-  El contacto fue anadido con exito\n",
    "-  Se anadio el contacto\n",
    "-  El email de juan perez es juanperez@mail.com\n",
    "\n",
    "Link de ayuda: https://github.com/cohere-ai/notebooks/blob/main/notebooks/agents/Vanilla_Tool_Use_v2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c6e43cb9-5e6a-4807-9818-c85408f1ba58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Su direcci√≥n de correo electr√≥nico es FOncativo@hotmail.com\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "\n",
    "co = cohere.ClientV2()\n",
    "\n",
    "def add_contact(name, phone, email):\n",
    "    \"\"\"\n",
    "    Agrega un contacto al diccionario.\n",
    "    Par√°metros:\n",
    "        name (str): Nombre del contacto.\n",
    "        phone (str): N√∫mero de tel√©fono del contacto.\n",
    "        email (str): Correo electr√≥nico del contacto.\n",
    "    Retorna:\n",
    "        str: Mensaje confirmando la adici√≥n del contacto.\n",
    "    \"\"\"\n",
    "    contacts[name] = {'phone': phone, 'email': email}\n",
    "    return \"Contacto a√±adido con √©xito.\"\n",
    "\n",
    "def get_information(name):\n",
    "    \"\"\"\n",
    "    Recupera la informaci√≥n de un contacto.\n",
    "    Par√°metros:\n",
    "        name (str): Nombre del contacto.\n",
    "    Retorna:\n",
    "        dict/str: Informaci√≥n del contacto o un mensaje si no existe.\n",
    "    \"\"\"\n",
    "    if name in contacts:\n",
    "        return contacts[name]\n",
    "    else:\n",
    "        return \"Contacto no encontrado.\"\n",
    "    \n",
    "contacts = {\n",
    "                        'Joaquin Lopez':{'tel': 15456663258, 'mail': 'Joacolocolopez@gmail.com'},\n",
    "                      'Flavio Oncativo':{'tel': 1545554178, 'mail': 'FOncativo@hotmail.com'}\n",
    "}\n",
    "\n",
    "aviable_fuctions = {\n",
    "    \"add_contact\":add_contact,\n",
    "    \"get_information\":get_information\n",
    "}\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\":\"function\",\n",
    "        \"function\":{\n",
    "            \"name\":\"add_contact\",\n",
    "            \"description\":\"Agregar los datos ingresados de una persona a la lista de contactos, agregando los datos nombre,telefono y email. Mostrar el mensaje que retorna\",\n",
    "            \"parameters\":{\n",
    "                \"type\":\"object\",\n",
    "                \"properties\":{\n",
    "                    \"name\":{\n",
    "                        \"type\":\"string\",\n",
    "                        \"description\":\"Nombre de la persona que se quiere registrar\"\n",
    "                    },\n",
    "                    \"phone\":{\n",
    "                        \"type\":\"integer\",\n",
    "                        \"description\":\"Numero de telefono de la persona que se quiere registrar\"\n",
    "                    },\n",
    "                    \"email\":{\n",
    "                        \"type\":\"string\",\n",
    "                        \"description\":\"Correo electronico o email de la persona que se quiere registrar\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\":[\"name\",\"phone\",\"email\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"type\":\"function\",\n",
    "        \"function\":{\n",
    "            \"name\":\"get_information\",\n",
    "            \"description\":\"Retorna los datos del nombre que este en la lista de contactos.\",\n",
    "            \"parameters\":{\n",
    "                \"type\":\"object\",\n",
    "                \"properties\":{\n",
    "                    \"name\":{\n",
    "                        \"type\":\"string\",\n",
    "                        \"description\":\"Nombre de la persona que se utiliza para la busqueda de sus datos en la lista de contactos\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\":[\"name\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "messages = [{\"role\":\"system\", \"content\":f\"\"\"Eres un asistente virtual. Tendras a disposicion funciones que te ayudaran a manejar solicitudes \n",
    "             de agregar o traer informacion de la persona que te indica. Cuando no te soliciten alguna de esas acciones puedes mantener una comunicacion \n",
    "             referido a la lista de contactos u otros temas\"\"\"},\n",
    "            {\"role\":\"user\", \"content\":\"Cual es el email de Flavio Oncativo?\"}]\n",
    "\n",
    "response_ = co.chat(\n",
    "    model=\"command-r-plus\",\n",
    "    messages=messages,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "for t in response_.message.tool_calls:\n",
    "    function_name = t.function.name\n",
    "    function_argument = json.loads(t.function.arguments)\n",
    "    tool_response = aviable_fuctions[function_name](**function_argument)\n",
    "\n",
    "\n",
    "response_final = co.chat(\n",
    "    model=\"command-r-plus\",\n",
    "    messages=[{\"role\":\"system\", \"content\":\"\"\"Eres un profesor de programacion, que es capaz de recibir cualquier tipo de dato o variable para manipularlos segun el prompt del usuario. \n",
    "               Debes ser capaz de leer prompts de usuarios e interpretar su intencion. Tu respuesta debe ser un mensaje sencillo que no supere las 20 palabras. \n",
    "               Recibiras la respuesta del modelo para que sigas la orientacion de su idea\"\"\"},\n",
    "              {\"role\":\"user\",\"content\":f\"usuario prompt: {messages[1]['content']}, datos encontrados: {tool_response}\"}]\n",
    "    \n",
    ")\n",
    "\n",
    "print(response_final.message.content[0].text)\n",
    "\n",
    "\n",
    "# La idea fue utilizar dos modelos porque el primero me solica indicar siempre que tipo de accion iba a utilizar,\n",
    "# entonces pense en poner otro modelo que pueda tomar los datos que le brida el primer modelo, como el resultado de la funcion y el primer prompt del user que le pasan)\n",
    "# y generar una corta oracion con la respuesta correcta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3bed9a-d5d1-49c3-b91b-8cd23bb9c9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddaf4d1-6ab9-4707-823d-11ada125b0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ef67d-dd54-41e7-bef5-f89ee575603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIPS\n",
    "# Probar primero generando una funcion y llamarla, luego anadir la otra\n",
    "# Plantearlo paso por paso en distintas celdas, analizar las salidas y las entradas, como identificamos a que funcion llamar?\n",
    "# luego automatizar dentro de una sola celda\n",
    "\n",
    "\n",
    "# Lo importante es entregar hasta donde lleguen, sea una funcion, las dos pero sin poder hacer el flujo automatico, lo que puedan, siempre y cuando este\n",
    "# claro lo que se quizo hacer con comentarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace85d2-cd00-4bd4-81e2-68113eb9f8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9d8badf-dc90-4005-b916-8e528105d797",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "\n",
    "Crear una funcion llamada \"history_answer\", que toma como parametro de entrada una pregunta sobre un contexto dado y la salida es la respuesta final del proceso impulsado por un LLM.\n",
    "\n",
    "Dada una historia, el usuario podra hacer preguntas sobre la misma y el LLM debe responder siguiendo los siguientes lineamientos:\n",
    "\n",
    "REQUISITOS DE LA RESPUESTA\n",
    "- las respuestas deben ser en base a la historia\n",
    "- ante la misma pregunta siempre debe responder de la misma manera.\n",
    "- que responda en solo una oracion.\n",
    "- el idioma que responde debe ser el mismo que con el que se pregunta (ingles, espanol, portugues).\n",
    "- que agregue emojis en la oracion que resuman el contenido de la misma.\n",
    "- que responda siempre en tercera persona.\n",
    "- si la pregunta no tiene relacion alguna con el contexto, la respuesta debe ser 'Lo siento no puedo ayudarte con eso'.\n",
    "- Responder con 'Hakuna Matata!' al final de **todas** las respuestas (no importa idioma ni cantidad de tokens).\n",
    "\n",
    "**Ayudin**: \n",
    "- No se limiten a usar 1 solo request al LLM, pueden dividirlo en partes para que por un lado se verifique el idioma, por otro lado se verifique si la pregunta tiene relacion con el contexto, etc\n",
    "\n",
    "- Estructuren bien el prompt procurando separar instrucciones, contexto(historia) y pregunta del usuario.\n",
    "\n",
    "- Recuerden usar el system message y user message.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f36820-b7d3-4813-a0c3-351c598106ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo flojo de estructura de prompt\n",
    "# prompt = f\"Responde a la pregunta: {pregunta} de manera concisa y divertida en base a la siguiente historia: {historia}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5602f56-4032-4cc6-a2c5-7b29cf2a2a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f3aaa967-d87f-45d9-9a72-6fc63afc931c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S√≠, un compa√±ero de Thomas cay√≥ ü•µ junto a √©l en el campo de batalla y ü™¶.\n",
      "\n",
      "Hakuna Matata!\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "\n",
    "co = cohere.ClientV2()\n",
    "\n",
    "historia = \"\"\"En un peque√±o feudo medieval, Thomas, un joven campesino de diecis√©is a√±os, trabajaba desde el amanecer en los campos de trigo del se√±or feudal. El sol apenas hab√≠a salido cuando √©l ya hab√≠a arado m√°s de lo que sus manos pod√≠an soportar. La vida era dura, pero su familia depend√≠a de la cosecha para pagar los impuestos y mantener su hogar de madera y paja.\n",
    "\n",
    "Un d√≠a, el feudo fue sacudido por noticias de guerra. El rey hab√≠a llamado a todos los hombres en edad de luchar. Thomas sab√≠a que, al igual que otros j√≥venes, no ten√≠a elecci√≥n. Cambi√≥ la hoz por una lanza rudimentaria y se uni√≥ a la milicia local. Sin entrenamiento, fue empujado a un campo de batalla embarrado, donde el acero resonaba y los gritos de los hombres llenaban el aire.\n",
    "\n",
    "La batalla fue un caos. Thomas, con el coraz√≥n latiendo en su pecho como un tambor de guerra, apenas pod√≠a distinguir amigo de enemigo. Logr√≥ esquivar una espada, pero cay√≥ al suelo, cubierto de lodo y sangre. Levant√°ndose, vio c√≥mo un compa√±ero ca√≠a junto a √©l, sus ojos abiertos, vac√≠os.\n",
    "\n",
    "Cuando la batalla termin√≥, el silencio era tan profundo como el vac√≠o que sent√≠a. Thomas regres√≥ al feudo, diferente, marcado por la muerte y la violencia. Su madre lo recibi√≥ con l√°grimas en los ojos, pero √©l, con la mirada fija en el horizonte, sab√≠a que la inocencia hab√≠a quedado atr√°s, enterrada en aquel campo de batalla. La paz del feudo ya no era la misma; √©l tampoco.\"\"\"\n",
    "\n",
    "def agregar_context(role=\"\",content=\"\"):\n",
    "    messages.append({\"role\":role, \"content\":content})\n",
    "\n",
    "\n",
    "def history_answer(pregunta):\n",
    "    # your code here\n",
    "    #Si la pregunta no tiene nada que ver con el contexto de la historia, debes responder obligatoriamente 'Lo siento no puedo ayudarte con eso.\n",
    "\n",
    "    sentido = co.chat(\n",
    "        model=\"command-r-plus\",\n",
    "        messages = [{\"role\":\"system\", \"content\":f\"\"\"Eres un experto en decifrar y puedes interpretar cualquier texto con su sentido mas pracmatico. \n",
    "                     Si desifras que la pregunta del usuario no tiene nada que ver con el contexto de la historia {historia}, \n",
    "                     responder obligatoriamente 'Lo siento no puedo ayudarte con eso'.\n",
    "                     Si la pregunta si tiene sentido con el contexto, solo responde que si\"\"\"},\n",
    "                    {\"role\":\"user\", \"content\":f\"Tiene sentido la pregunta '{pregunta}' en el contexto de la historia?\"}]\n",
    "    )\n",
    "\n",
    "    sentido_resp = sentido.message.content[0].text\n",
    "    #print(sentido_resp)\n",
    "\n",
    "    traductor = co.chat(\n",
    "        model=\"command-r-plus\",\n",
    "        messages = [{\"role\":\"system\", \"content\":\"Eres un asistente especializado en traduccion de idiomas. Tu respuesta solo tiene que ser una palabra con el idioma que debe utilizarse\"},\n",
    "                    {\"role\":\"user\", \"content\":f\"Segun el idioma en la que se encuentra escrita la pregunta '{pregunta}', debes indicarle al modelo en que idioma debe efectuar su pregunta, para que esten sincronizados los idiomas'\"}]\n",
    "    )\n",
    "\n",
    "    traductor_resp = traductor.message.content[0].text\n",
    "    #print(traductor_resp)\n",
    "\n",
    "    buscador = co.chat(\n",
    "        model=\"command-r-plus\",\n",
    "        messages=[{\"role\":\"system\",\"content\":f\"\"\"Sos un asistente experto en programacion y busqueda. Debes ser capaz de buscar la pregunta en el historial {messages}  del modelo y responder si o no. \n",
    "                   .Si tu respuesta a la pregunta es 'S√≠', devu√©lveme una variable 'afirmacion' con el valor 'SI' y otra variable 'respuesta' con la respuesta encontrada en la lista.\n",
    "                   Si vez varias respuestas sobre esa pregunta, contestame siempre con la ultima. \n",
    "                   Si tu respuesta es 'NO', mostrame solamente una variable 'afirmacion' con el valor 'NO'.\n",
    "                   Responde √∫nicamente con la lista del diccionario en formato Python, sin texto adicional.\"\"\"},\n",
    "                  {\"role\":\"user\",\"content\":f\"La {pregunta} esta repetida en el historial?\"}]\n",
    "    )\n",
    "\n",
    "    resp_buscador = buscador.message.content[0].text\n",
    "    #print(resp_buscador)\n",
    "    \n",
    "\n",
    "    response_1 = co.chat(\n",
    "        model=\"command-r-plus\",\n",
    "        messages = [{\"role\":\"system\",\"content\":f\"\"\"Eres un profesor de Literatura y Letras y tienes que interpretar el texto: {historia}. \n",
    "                     Tienes la capacidad de poder leer cualquier fracmento de texto y poder estudiarlo para tener conocimiento sobre informacion que brinde el texto, \n",
    "                     para poder responder cualquier tipo de pregunta hacia el texto. Tu respuesta tiene que ser en una oracion.\n",
    "                     Debes tener en cuenta la respuesta del modelo '{traductor_resp}' que te idicara con que idioma debes responder. Es obligatorio hacerle caso.\n",
    "                     Presta atencion a la respuesta del modelo '{sentido_resp}' porque si su respuesta es 'Lo siento, no puedo ayudarte con eso.' debes responder solamente eso obligatoriamente.\n",
    "                     Debes tener en cuenta, si el modelo anterior de busqueda encuentra la pregunta repetida '{resp_buscador}', debes responder lo que el te indique. Si no esta repetida, se libre de contestar'\"\"\"},\n",
    "                    {\"role\":\"user\",\"content\":f\"La pregunta del usuario es la siguiente: {pregunta}\"}]\n",
    "    )\n",
    "\n",
    "    respuesta_model_1 = response_1.message.content[0].text\n",
    "    #print(respuesta_model_1)\n",
    "\n",
    "    agregar_context(\"user\",f\"{pregunta}\")\n",
    "    agregar_context(\"assistant\",f\"{respuesta_model_1}\")\n",
    "\n",
    "\n",
    "  \n",
    "    emojin = co.chat(\n",
    "        model = \"command-r-plus\",\n",
    "        messages = [{\"role\":\"system\", \"content\":\"Eres un chatbot. Debes contemplar el manejo de emojins, que puedas remplazar una palabra con un emojin que signifique o simbolice la palabra que vos elijas\"},\n",
    "                    {\"role\":\"user\", \"content\":f\"Elige dos palabras que signifiquen verbos de esta oracion y remplazalas con emojins: {respuesta_model_1}\"}]\n",
    "    )\n",
    "\n",
    "    respuesta_emojin= emojin.message.content[0].text\n",
    "    #print(respuesta_emojin)\n",
    "\n",
    "\n",
    "    tercera_persona = co.chat(\n",
    "        model = \"command-r-plus\",\n",
    "        messages = [{\"role\":\"system\", \"content\":f\"\"\"Eres experto en literatura.Tus respuestas se deben configurar siempre en tercera persona. Al final de cada respuesta debes obligatoriamente incluir la frase 'Hakuna Matata! al final de la oracion. Tu respuesta debe ser solamente la oracion en tercera persona.\n",
    "                     Si la respuesta del modelo '{sentido_resp} es 'Lo siento, no puedo ayudarte con eso.' debes responder solamente eso obligatoriamente y quitando los emojins.\"\"\"},\n",
    "                    {\"role\":\"user\", \"content\":f\"Convierte esta oracion: {respuesta_emojin} sin la agregar alguna extencion en tu respuesta y sin quitar los emojins\"}]\n",
    "    )\n",
    "\n",
    "    respuesta_final= tercera_persona.message.content[0].text\n",
    "    #print(respuesta_final)\n",
    "\n",
    "    return respuesta_final\n",
    "\n",
    "\n",
    "pregunta = \"Alguien murio en la historia?\"\n",
    "\n",
    "\n",
    "# respuesta\n",
    "print(history_answer(pregunta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f246d9-ec91-498a-a06a-172947316325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795f451e-6c12-4631-895f-fa6ac8f16b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db6e9a7d-aa39-4a01-ba55-9a7f4ea39522",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "\n",
    "Crear un chatbot sencillo impulsado por un LLM. \n",
    "\n",
    "Dicho bot esta destinado a un usuario final y debe cumplir las siguientes **condiciones en sus respuestas**:\n",
    "\n",
    "- Responder en no mas de 70 tokens.\n",
    "- Responder de manera positiva, con un tono entusiasta.\n",
    "- Responder con consejos √∫tiles, como si fueras un tutor.\n",
    "\n",
    " \n",
    "**Otras consideraciones**:\n",
    "\n",
    "Respetar el formato de la interfaz provista por el ejercicio.\n",
    "\n",
    "Ademas agregar al codigo propuesto un historial de conversaciones para que el bot pueda mantener el hilo de lo que se esta hablando. Para probar no usen mas de 3 conversaciones anidadas para no enviarle tantos tokens.\n",
    "\n",
    "Dejar impreso en el notebook el historial de la conversacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5284210f-23a3-4db1-b315-db724a3bb3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "aa475fa8-e48b-423e-9006-7478a462129c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ac43cdcee14b0ebea82e5231760794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Escribe tu mensaje aqu√≠...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea9b206e0c94cf79c7e87b2b766fda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Enviar', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e71266ef7348ad9e4c7ad5cd5890cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cohere\n",
    "\n",
    "co = cohere.ClientV2()\n",
    "chat_history = []\n",
    "\n",
    "def agregar_context(role=\"\",content=\"\"):\n",
    "    chat_history.append({\"role\":role, \"content\":content})\n",
    "\n",
    "# Crear widgets de entrada y salida\n",
    "input_box = widgets.Text(placeholder='Escribe tu mensaje aqu√≠...')\n",
    "send_button = widgets.Button(description='Enviar')\n",
    "output_box = widgets.Output()\n",
    "\n",
    "\n",
    "\n",
    "# Funci√≥n de respuesta simulada del chatbot\n",
    "def chatbot_response(message):\n",
    "    # Aqu√≠ puedes conectar tu modelo o l√≥gica de chatbot real\n",
    "    model1 = co.chat(\n",
    "        model = \"command-r-plus\",\n",
    "        messages = [{\"role\":\"system\", \"content\":\"\"\"Eres un chatbot que logra mantener conversaciones amigables y preparado para hablar del tema que se trate.\n",
    "                     Tus respuestas deben ser siempre de forma positiva, muestrate entusiasmado en tu respuesta. Debes responder a la altura de un profesor con experiencia de vida, capaz de dar consejos todo el tiempo.\n",
    "                     Debes conjugar una oracion que tenga sentido con la charla proporcionada y gramaticalmente bien escrita.\"\"\"},\n",
    "                     {\"role\":\"user\", \"content\":f\"{message}\"}],\n",
    "        max_tokens=70\n",
    "    \n",
    "    )\n",
    "\n",
    "    respuesta_model1= model1.message.content[0].text\n",
    "\n",
    "    model2 = co.chat(\n",
    "        model = \"command-r-plus\",\n",
    "        messages = [{\"role\":\"system\", \"content\":\"\"\"Comportate como un asistente virtual especializado en reconocer en el texto proporcionado, si logra ser un texto con un formato de respuesta positiva o no.\n",
    "                     En caso de no ser positiva la respuesta, genera tu una modificacion en su gramatica para que se lea un texto entusiasta.\n",
    "                     Si la respuesta del modelo es totalmente positiva, toma ese texto y muestrame solamente ese texto\"\"\"},\n",
    "                     {\"role\":\"user\", \"content\":f\"{respuesta_model1}\"}],\n",
    "        max_tokens=70\n",
    "    \n",
    "    )\n",
    "\n",
    "    respuesta_model2= model2.message.content[0].text\n",
    "\n",
    "    agregar_context(\"user\",f\"{message}\")\n",
    "    agregar_context(\"assistant\", f\"{respuesta_model2}\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    return respuesta_model2\n",
    "\n",
    "# Funci√≥n de manejo del bot√≥n\n",
    "def on_send_button_clicked(b):\n",
    "    with output_box:\n",
    "        clear_output(wait=True)\n",
    "        user_message = input_box.value\n",
    "        if user_message.strip():\n",
    "            print(f\"T√∫: {user_message}\")\n",
    "            response = chatbot_response(user_message)\n",
    "            print(f\"Chatbot: {response}\")\n",
    "        input_box.value = ''\n",
    "\n",
    "# Asociar funci√≥n al bot√≥n\n",
    "send_button.on_click(on_send_button_clicked)\n",
    "\n",
    "# Mostrar widgets\n",
    "display(input_box, send_button, output_box)\n",
    "print(chat_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436d5bce-0189-4ecd-a06d-fcc7a81315b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a213283-3ef7-4df8-8b7f-3bb11d58362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(conversation_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fb6d6a-1c32-42e5-a1ee-d62b2bc0785a",
   "metadata": {},
   "source": [
    "### RECOMENDACIONES GENERALES\n",
    "\n",
    "No se confien probando con un par de respuestas y ya, hagan minimo 5 pruebas por ejercicio para asi tener mas chances de visualizar errores en la generacion del contenido.\n",
    "\n",
    "Prueben combinar LLMs con programacion convencional para los casos que vean convenientes (decisiones if else, respuestas estaticas, etc)\n",
    "\n",
    "Prueben con distintos modelos de Cohere, hay algunos optimizados para ciertas aplicaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f2e37-bf32-43b7-8958-e39954a20fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ent_pi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
